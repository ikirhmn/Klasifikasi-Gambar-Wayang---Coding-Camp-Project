# -*- coding: utf-8 -*-
"""Copy of Template Submission Akhir.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19JD8xUaSlkTK9bvlNUSqhSOQsJmSWXfH

# Proyek Klasifikasi Gambar: [Input Nama Dataset]
- **Nama:** Rizki Rahman Maulana
- **Email:** mc253d5y0335@student.devacademy.id
- **ID Dicoding:** ikirahman

## Import Semua Packages/Library yang Digunakan

## Data Preparation
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
import shutil
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import itertools
import random
from google.colab import drive
from google.colab import files

"""### Data Loading"""

drive.mount('/content/drive')

import zipfile

local_zip = '/content/drive/MyDrive/Dataset/wayang.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/wayang')
zip_ref.close()

"""### Data Preprocessing

#### Split Dataset
"""

# Path asli dataset
original_dataset_dir = '/content/wayang/labeled-indonesian-wayang'

# Path baru setelah split
base_dir = '/content/wayang_split'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

# Membuat direktori
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Dapatkan semua class (folder di /wayang)
classes = os.listdir(original_dataset_dir)

# Loop untuk semua class
for class_name in classes:
    class_path = os.path.join(original_dataset_dir, class_name)
    if os.path.isdir(class_path):
        # Buat subfolder untuk setiap class
        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

        # Ambil semua file gambar dalam class
        images = os.listdir(class_path)
        random.shuffle(images)

        # Hitung jumlah data
        total_images = len(images)
        train_split = int(0.7 * total_images)
        validation_split = int(0.2 * total_images)

        # Copy gambar ke masing-masing folder
        for idx, image_name in enumerate(images):
            src_path = os.path.join(class_path, image_name)

            if idx < train_split:
                dst_path = os.path.join(train_dir, class_name, image_name)
            elif idx < train_split + validation_split:
                dst_path = os.path.join(validation_dir, class_name, image_name)
            else:
                dst_path = os.path.join(test_dir, class_name, image_name)

            shutil.copy2(src_path, dst_path)

print("Dataset berhasil di-split 70% Train, 20% Validation, 10% Test! ðŸš€")

"""## Modelling"""

# ImageDataGenerator untuk augmentasi pada training set
train_datagen = ImageDataGenerator(
    rescale=1./255,              # Normalisasi pixel 0-255 jadi 0-1
    rotation_range=10,           # Rotasi acak
    width_shift_range=0.2,       # Geser lebar
    height_shift_range=0.2,      # Geser tinggi
    shear_range=0.2,             # Shear
    zoom_range=0.2,              # Zoom
    vertical_flip=True,          # Flip vertikal
)

# Untuk validation dan test, cukup rescale saja
test_val_datagen = ImageDataGenerator(rescale=1./255)

# Load data
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),       # Resize ke 150x150
    batch_size=32,
    class_mode='categorical'
)

validation_generator = test_val_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_val_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    shuffle=False                # Penting saat evaluasi
)

# Model Sequential dengan tambahan BatchNormalization dan penambahan unit di Dense layer
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3), kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(256, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(train_generator.num_classes, activation='softmax')  # Output layer
])

# Compile model
model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

model.summary()

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=0.0001,
    verbose=1
)

history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=50,
    callbacks=[early_stopping, lr_scheduler]
)

"""## Evaluasi dan Visualisasi"""

# Buat plot akurasi
plt.figure(figsize=(14, 5))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Accuracy per Epoch', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend()
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Loss per Epoch', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## Konversi Model"""

export_dir = 'exports'
saved_model_dir = os.path.join(export_dir, 'saved_model')
tflite_model_path = os.path.join(export_dir, 'model.tflite')

if os.path.exists(export_dir):
    shutil.rmtree(export_dir)
os.makedirs(saved_model_dir, exist_ok=True)

# 1. Save model menggunakan tf.saved_model.save untuk SavedModel
tf.saved_model.save(model, saved_model_dir)
print(f"âœ… Model berhasil disimpan dalam format SavedModel di: {saved_model_dir}")

# 2. Convert ke TFLite
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

# Save TFLite model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)
print(f"âœ… Model berhasil dikonversi ke TFLite dan disimpan di: {tflite_model_path}")

print("\nðŸš€ SavedModel dan TFLite sukses! Siap ke TFJS kalau mau lanjut!")


"""## Inference (Optional)

### Inference SavedModel
"""

loaded_model = tf.saved_model.load('exports/saved_model')
infer = loaded_model.signatures["serving_default"]

image_path = '/content/wayang_split/test/anoman/anoman002b.jpg'
img = Image.open(image_path).resize((150, 150))
img_array = np.array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

img_array = img_array.astype(np.float32)

predictions = infer(tf.constant(img_array))

probabilities = predictions['output_0'].numpy()[0]

class_names = list(train_generator.class_indices.keys())

for i, probability in enumerate(probabilities):
    print(f"{class_names[i]}: {probability * 100:.2f}%")

predicted_class_index = np.argmax(probabilities)
predicted_class_name = class_names[predicted_class_index]

print(f"\nPredicted class: {predicted_class_name}")

def preprocess_image(uploaded_file):
    try:
        image = Image.open(uploaded_file).convert('RGB').resize((150, 150))
        img_array = np.array(image) / 255.0
        img_array = np.expand_dims(img_array, axis=0)
        img_array = img_array.astype(np.float32)
        return img_array
    except Exception as e:
        print(f"Error preprocessing image: {e}")
        return None

uploaded = files.upload()

for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))

    image_array = preprocess_image(fn)

    if image_array is not None:
        predictions = infer(tf.constant(image_array))
        probabilities = predictions['output_0'].numpy()[0]

        class_names = list(train_generator.class_indices.keys())

        for i, probability in enumerate(probabilities):
            print(f"{class_names[i]}: {probability * 100:.2f}%")

        predicted_class_index = np.argmax(probabilities)
        predicted_class_name = class_names[predicted_class_index]

        print(f"\nPredicted class: {predicted_class_name}")

